{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "langs=[\"cs\",\"de\",\"en\",\"it\",\"nl\",\"sk\"]\n",
    "data_cs= load_dataset(\"multi_eurlex\", language=\"cs\")\n",
    "data_de= load_dataset(\"multi_eurlex\", language=\"de\")\n",
    "data_en= load_dataset(\"multi_eurlex\", language=\"en\")\n",
    "data_it= load_dataset(\"multi_eurlex\", language=\"it\")\n",
    "data_nl= load_dataset(\"multi_eurlex\", language=\"nl\")\n",
    "data_sk= load_dataset(\"multi_eurlex\", language=\"sk\")\n",
    "\n",
    "#data_train=concatenate_datasets([data_cs['train'],data_de[\"train\"],data_en[\"train\"],data_it[\"train\"],data_nl[\"train\"],data_sk[\"train\"]])\n",
    "#data_test=concatenate_datasets([data_cs['test'],data_de[\"test\"],data_en[\"test\"],data_it[\"test\"],data_nl[\"test\"],data_sk[\"test\"]])\n",
    "\n",
    "data_train = concatenate_datasets([data_de['train'], data_en['train'], data_it['train'], data_nl['train']])\n",
    "data_test = concatenate_datasets([data_cs['train'], data_sk['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import shuffle\n",
    "LABEL_MAP=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]  ##level 1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertForSequenceClassification\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        \n",
    "        self.num_labels = args.num_labels\n",
    "        self.outer_batch_size = args.outer_batch_size\n",
    "        self.inner_batch_size = args.inner_batch_size\n",
    "        self.outer_update_lr  = args.outer_update_lr\n",
    "        self.inner_update_lr  = args.inner_update_lr\n",
    "        self.inner_update_step = args.inner_update_step\n",
    "        self.inner_update_step_eval = args.inner_update_step_eval\n",
    "        self.bert_model = args.bert_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
    "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, batch_tasks, training = True):\n",
    "        \"\"\"\n",
    "        batch = [(support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset)]\n",
    "        \n",
    "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
    "        \"\"\"\n",
    "        task_accs = []\n",
    "        sum_gradients = []\n",
    "        num_task = len(batch_tasks)\n",
    "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
    "\n",
    "        for task_id, task in enumerate(batch_tasks):\n",
    "            support = task[0]\n",
    "            query   = task[1]\n",
    "            \n",
    "            fast_model = deepcopy(self.model)\n",
    "            fast_model.to(self.device)\n",
    "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
    "                                            batch_size=self.inner_batch_size)\n",
    "            \n",
    "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
    "            fast_model.train()\n",
    "            \n",
    "            print('----Task',task_id, '----')\n",
    "            for i in range(0,num_inner_update_step):\n",
    "                all_loss = []\n",
    "                for inner_step, batch in enumerate(support_dataloader):\n",
    "                    \n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
    "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
    "                    \n",
    "                    loss = outputs[0]              \n",
    "                    loss.backward()\n",
    "                    inner_optimizer.step()\n",
    "                    inner_optimizer.zero_grad()\n",
    "                    \n",
    "                    all_loss.append(loss.item())\n",
    "                \n",
    "                if i % 4 == 0:\n",
    "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
    "\n",
    "            query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
    "            query_batch = iter(query_dataloader).next()\n",
    "            query_batch = tuple(t.to(self.device) for t in query_batch)\n",
    "            q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
    "            q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
    "            \n",
    "            if training:\n",
    "                q_loss = q_outputs[0]\n",
    "                q_loss.backward()\n",
    "                fast_model.to(torch.device('cpu'))\n",
    "                for i, params in enumerate(fast_model.parameters()):\n",
    "                    if task_id == 0:\n",
    "                        sum_gradients.append(deepcopy(params.grad))\n",
    "                    else:\n",
    "                        sum_gradients[i] += deepcopy(params.grad)\n",
    "\n",
    "            q_logits = F.softmax(q_outputs[1],dim=1)\n",
    "            pre_label_id = torch.argmax(q_logits,dim=1)\n",
    "            pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
    "            q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            acc = accuracy_score(pre_label_id,q_label_id)\n",
    "            task_accs.append(acc)\n",
    "            \n",
    "            del fast_model, inner_optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if training:\n",
    "            # Average gradient across tasks\n",
    "            for i in range(0,len(sum_gradients)):\n",
    "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
    "\n",
    "            #Assign gradient for original model, then using optimizer to update its weights\n",
    "            for i, params in enumerate(self.model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "\n",
    "            self.outer_optimizer.step()\n",
    "            self.outer_optimizer.zero_grad()\n",
    "            \n",
    "            del sum_gradients\n",
    "            gc.collect()\n",
    "        \n",
    "        return np.mean(task_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, random\n",
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    #idxs = list(range(0,512))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "    for i in range(0,len(idxs), batch_size):\n",
    "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        self.num_labels = 21\n",
    "        self.meta_epoch=10\n",
    "        self.k_spt=80\n",
    "        self.k_qry=20\n",
    "        self.outer_batch_size = 2    #change\n",
    "        self.inner_batch_size = 12\n",
    "        self.outer_update_lr = 0.001  #Change\n",
    "        self.inner_update_lr = 0.001\n",
    "        self.inner_update_step = 10\n",
    "        self.inner_update_step_eval = 40\n",
    "        self.bert_model = 'xlm-roberta-base'\n",
    "        self.num_task_train = 500\n",
    "        self.num_task_test = 5\n",
    "\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_resource=['cs','sk']\n",
    "train_examples=[r for r in data_train]\n",
    "test_examples=[r for r in data_test]\n",
    "#type(train_examples)\n",
    "#len(train_examples)\n",
    "#print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "class MetaTask(Dataset):\n",
    "    \n",
    "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
    "        \"\"\"\n",
    "        :param samples: list of samples\n",
    "        :param num_task: number of training tasks.\n",
    "        :param k_support: number of support sample per task\n",
    "        :param k_query: number of query sample per task\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        random.shuffle(self.examples)\n",
    "        \n",
    "        self.num_task = num_task\n",
    "        self.k_support = k_support\n",
    "        self.k_query = k_query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = 200       #\n",
    "        self.create_batch(self.num_task)\n",
    "    \n",
    "    def create_batch(self, num_task):\n",
    "        self.supports = []  # support set\n",
    "        self.queries = []  # query set\n",
    "        \n",
    "        for b in range(num_task):  # for each task\n",
    "            # 1.select domain randomly\n",
    "            #domain = random.choice(self.examples)['domain']   #domain corresponds to low resource language\n",
    "            #domainExamples = [e for e in self.examples if e['domain'] == domain] \n",
    "            #domainExamples=random.choice(self.examples)\n",
    "            domainExamples=random.sample(self.examples, 1000)\n",
    "            # 1.select k_support + k_query examples from domain randomly\n",
    "            selected_examples = random.sample(list(domainExamples),self.k_support + self.k_query)\n",
    "            random.shuffle(selected_examples)\n",
    "            exam_train = selected_examples[:self.k_support]\n",
    "            exam_test  = selected_examples[self.k_support:]\n",
    "            \n",
    "            self.supports.append(exam_train)\n",
    "            self.queries.append(exam_test)\n",
    "\n",
    "    def create_feature_set(self,examples):\n",
    "        all_input_ids      = torch.empty((len(examples), self.max_seq_length), dtype = torch.long)\n",
    "        all_attention_mask = torch.empty((len(examples), self.max_seq_length), dtype = torch.long)\n",
    "        all_segment_ids    = torch.empty((len(examples), self.max_seq_length), dtype = torch.long)\n",
    "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
    "\n",
    "        for id_,example in enumerate(examples):\n",
    "            input_ids = tokenizer.encode(example['text'][:self.max_seq_length])\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            segment_ids    = [0] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "            #while len(input_ids) < len(example['text']):\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            #label_id = LABEL_MAP[example['label']].  ##check labels\n",
    "            label_id = LABEL_MAP[example['labels'][0]]\n",
    "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
    "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
    "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
    "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
    "\n",
    "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
    "        return tensor_set\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        support_set = self.create_feature_set(self.supports[index])\n",
    "        query_set   = self.create_feature_set(self.queries[index])\n",
    "        return support_set, query_set\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.num_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745fe7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "train = MetaTask(train_examples, num_task = 100, k_support=100, k_query=30, tokenizer = tokenizer)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(args)\n",
    "    \n",
    "test = MetaTask(test_examples, num_task = args.num_task_test, k_support=args.k_spt,k_query=args.k_qry, tokenizer = tokenizer)\n",
    "\n",
    "global_step = 0\n",
    "#for epoch in range(args.epoch):\n",
    "for epoch in range(args.meta_epoch):\n",
    "        train = MetaTask(train_examples, num_task = args.num_task_train, k_support=args.k_spt, \n",
    "                         k_query=args.k_qry, tokenizer = tokenizer)\n",
    "\n",
    "        db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "\n",
    "        for step, task_batch in enumerate(db):\n",
    "\n",
    "            acc = learner(task_batch)\n",
    "\n",
    "            print('Step:', step, '\\ttraining Acc:', acc)\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                random_seed(123)\n",
    "                print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "                db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "                acc_all_test = []\n",
    "\n",
    "                for test_batch in db_test:\n",
    "                    acc = learner(test_batch, training = False)\n",
    "                    acc_all_test.append(acc)\n",
    "\n",
    "                print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "\n",
    "                random_seed(int(time.time() % 10))\n",
    "\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649e33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
