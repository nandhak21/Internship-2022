{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "480d2f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-language=cs\n",
      "Reusing dataset multi_eurlex (/home/ubuntu/.cache/huggingface/datasets/multi_eurlex/default-language=cs/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Using custom data configuration default-language=de\n",
      "Reusing dataset multi_eurlex (/home/ubuntu/.cache/huggingface/datasets/multi_eurlex/default-language=de/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Using custom data configuration default-language=en\n",
      "Reusing dataset multi_eurlex (/home/ubuntu/.cache/huggingface/datasets/multi_eurlex/default-language=en/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Using custom data configuration default-language=it\n",
      "Reusing dataset multi_eurlex (/home/ubuntu/.cache/huggingface/datasets/multi_eurlex/default-language=it/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Using custom data configuration default-language=nl\n",
      "Reusing dataset multi_eurlex (/home/ubuntu/.cache/huggingface/datasets/multi_eurlex/default-language=nl/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n",
      "Using custom data configuration default-language=sk\n",
      "Reusing dataset multi_eurlex (/home/ubuntu/.cache/huggingface/datasets/multi_eurlex/default-language=sk/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "langs=[\"cs\",\"de\",\"en\",\"it\",\"nl\",\"sk\"]\n",
    "data_cs= load_dataset(\"multi_eurlex\", language=\"cs\",split='train[:200]')\n",
    "data_de= load_dataset(\"multi_eurlex\", language=\"de\",split='train[:1000]')\n",
    "data_en= load_dataset(\"multi_eurlex\", language=\"en\",split='train[:1000]')\n",
    "data_it= load_dataset(\"multi_eurlex\", language=\"it\",split='train[:1000]')\n",
    "data_nl= load_dataset(\"multi_eurlex\", language=\"nl\",split='train[:1000]')\n",
    "data_sk= load_dataset(\"multi_eurlex\", language=\"sk\",split='train[:200]')\n",
    "\n",
    "#data_train=concatenate_datasets([data_cs['train'],data_de[\"train\"],data_en[\"train\"],data_it[\"train\"],data_nl[\"train\"],data_sk[\"train\"]])\n",
    "#data_test=concatenate_datasets([data_cs['test'],data_de[\"test\"],data_en[\"test\"],data_it[\"test\"],data_nl[\"test\"],data_sk[\"test\"]])\n",
    "\n",
    "data_train = concatenate_datasets([data_de, data_en, data_it, data_nl])\n",
    "data_test = concatenate_datasets([data_cs, data_sk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d031b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import shuffle\n",
    "LABEL_MAP=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]  ##level 1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e865447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertForSequenceClassification\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        \n",
    "        self.num_labels = args.num_labels\n",
    "        self.outer_batch_size = args.outer_batch_size\n",
    "        self.inner_batch_size = args.inner_batch_size\n",
    "        self.outer_update_lr  = args.outer_update_lr\n",
    "        self.inner_update_lr  = args.inner_update_lr\n",
    "        self.inner_update_step = args.inner_update_step\n",
    "        self.inner_update_step_eval = args.inner_update_step_eval\n",
    "        self.bert_model = args.bert_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
    "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
    "        self.model.train()\n",
    "\n",
    "    def forward(self, batch_tasks, training = True):\n",
    "        \"\"\"\n",
    "        batch = [(support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset),\n",
    "                 (support TensorDataset, query TensorDataset)]\n",
    "        \n",
    "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
    "        \"\"\"\n",
    "        task_accs = []\n",
    "        sum_gradients = []\n",
    "        num_task = len(batch_tasks)\n",
    "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
    "\n",
    "        for task_id, task in enumerate(batch_tasks):\n",
    "            support = task[0]\n",
    "            query   = task[1]\n",
    "            \n",
    "            fast_model = deepcopy(self.model)\n",
    "            fast_model.to(self.device)\n",
    "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
    "                                            batch_size=self.inner_batch_size)\n",
    "            \n",
    "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
    "            fast_model.train()\n",
    "            \n",
    "            print('----Task',task_id, '----')\n",
    "            for i in range(0,num_inner_update_step):\n",
    "                all_loss = []\n",
    "                for inner_step, batch in enumerate(support_dataloader):\n",
    "                    \n",
    "                    batch = tuple(t.to(self.device) for t in batch)\n",
    "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
    "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
    "                    \n",
    "                    loss = outputs[0]              \n",
    "                    loss.backward()\n",
    "                    inner_optimizer.step()\n",
    "                    inner_optimizer.zero_grad()\n",
    "                    \n",
    "                    all_loss.append(loss.item())\n",
    "                \n",
    "                if i % 4 == 0:\n",
    "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
    "\n",
    "            query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
    "            query_batch = iter(query_dataloader).next()\n",
    "            query_batch = tuple(t.to(self.device) for t in query_batch)\n",
    "            q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
    "            q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
    "            \n",
    "            if training:\n",
    "                q_loss = q_outputs[0]\n",
    "                q_loss.backward()\n",
    "                fast_model.to(torch.device('cpu'))\n",
    "                for i, params in enumerate(fast_model.parameters()):\n",
    "                    if task_id == 0:\n",
    "                        sum_gradients.append(deepcopy(params.grad))\n",
    "                    else:\n",
    "                        sum_gradients[i] += deepcopy(params.grad)\n",
    "\n",
    "            q_logits = F.softmax(q_outputs[1],dim=1)\n",
    "            pre_label_id = torch.argmax(q_logits,dim=1)\n",
    "            pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
    "            q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            acc = accuracy_score(pre_label_id,q_label_id)\n",
    "            task_accs.append(acc)\n",
    "            \n",
    "            del fast_model, inner_optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if training:\n",
    "            # Average gradient across tasks\n",
    "            for i in range(0,len(sum_gradients)):\n",
    "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
    "\n",
    "            #Assign gradient for original model, then using optimizer to update its weights\n",
    "            for i, params in enumerate(self.model.parameters()):\n",
    "                params.grad = sum_gradients[i]\n",
    "\n",
    "            self.outer_optimizer.step()\n",
    "            self.outer_optimizer.zero_grad()\n",
    "            \n",
    "            del sum_gradients\n",
    "            gc.collect()\n",
    "        \n",
    "        return np.mean(task_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c46afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, random\n",
    "def random_seed(value):\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    np.random.seed(value)\n",
    "    random.seed(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0a3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
    "    idxs = list(range(0,len(taskset)))\n",
    "    #idxs = list(range(0,512))\n",
    "    if is_shuffle:\n",
    "        random.shuffle(idxs)\n",
    "    for i in range(0,len(idxs), batch_size):\n",
    "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "648a08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArgs:\n",
    "    def __init__(self):\n",
    "        self.num_labels = 21\n",
    "        self.meta_epoch=10\n",
    "        self.k_spt=80\n",
    "        self.k_qry=20\n",
    "        self.outer_batch_size = 2    #change\n",
    "        self.inner_batch_size = 12\n",
    "        self.outer_update_lr = 0.001  #Change\n",
    "        self.inner_update_lr = 0.001\n",
    "        self.inner_update_step = 10\n",
    "        self.inner_update_step_eval = 40\n",
    "        self.bert_model = 'xlm-roberta-base'\n",
    "        self.num_task_train = 500\n",
    "        self.num_task_test = 5\n",
    "\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cd3aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "4000\n",
      "{'celex_id': '32006D0213', 'text': 'ENTSCHEIDUNG DER KOMMISSION\\nvom 6. März 2006\\nzur Festlegung der Brandverhaltensklassen für bestimmte Bauprodukte (Holzfußböden sowie Wand- und Deckenbekleidungen aus Massivholz)\\n(Bekannt gegeben unter Aktenzeichen K(2006) 655)\\n(Text von Bedeutung für den EWR)\\n(2006/213/EG)\\nDIE KOMMISSION DER EUROPÄISCHEN GEMEINSCHAFTEN -\\ngestützt auf den Vertrag zur Gründung der Europäischen Gemeinschaft,\\ngestützt auf die Richtlinie 89/106/EWG des Rates vom 21. Dezember 1988 zur Angleichung der Rechts- und Verwaltungsvorschriften der Mitgliedstaaten über Bauprodukte (1), insbesondere auf Artikel 20 Absatz 2,\\nin Erwägung nachstehender Gründe:\\n(1)\\nNach der Richtlinie 89/106/EWG kann es zur Berücksichtigung der auf einzelstaatlicher, regionaler oder lokaler Ebene bestehenden unterschiedlichen Schutzniveaus für Bauwerke erforderlich sein, dass in den Grundlagendokumenten Klassen entsprechend der Leistung des jeweiligen Produkts im Hinblick auf die jeweilige wesentliche Anforderung festgelegt werden. Diese Dokumente wurden in Form einer Mitteilung der Kommission über die Grundlagendokumente der Richtlinie 89/106/EWG des Rates (2) veröffentlicht.\\n(2)\\nFür die wesentliche Anforderung „Brandschutz“ enthält das Grundlagendokument Nr. 2 eine Reihe zusammenhängender Maßnahmen, die gemeinsam die Strategie für den Brandschutz festlegen, die dann in den Mitgliedstaaten in unterschiedlicher Weise entwickelt werden kann.\\n(3)\\nDas Grundlagendokument Nr. 2 nennt als eine dieser Maßnahmen die Begrenzung der Entstehung und Ausbreitung von Feuer und Rauch in einem gegebenen Bereich, indem das Potenzial der Bauprodukte, zu einem Vollbrand beizutragen, begrenzt wird.\\n(4)\\nDas Grenzniveau kann nur in Form unterschiedlicher Stufen des Brandverhaltens der Bauprodukte in ihrer Endanwendung ausgedrückt werden.\\n(5)\\nAls harmonisierte Lösung wurde in der Entscheidung 2000/147/EG der Kommission vom 8. Februar 2000 zur Durchführung der Richtlinie 89/106/EWG des Rates im Hinblick auf die Klassifizierung des Brandverhaltens von Bauprodukten (3) ein System von Klassen festgelegt.\\n(6)\\nFür Holzfußböden sowie Wand- und Deckenbekleidungen aus Massivholz muss die mit der Entscheidung 2000/147/EG festgelegte Klassifizierung verwendet werden.\\n(7)\\nDas Brandverhalten zahlreicher Bauprodukte/-materialien im Rahmen der in der Entscheidung 2000/147/EG festgelegten Klassifizierung ist so eindeutig ermittelt und den für die Brandschutzvorschriften zuständigen Stellen in den Mitgliedstaaten so gut bekannt, dass sich eine Prüfung dieses Leistungsmerkmals erübrigt.\\n(8)\\nDie in dieser Entscheidung vorgesehenen Maßnahmen entsprechen der Stellungnahme des Ständigen Ausschusses für das Bauwesen -\\nHAT FOLGENDE ENTSCHEIDUNG ERLASSEN:\\nArtikel 1\\nDie Bauprodukte und/oder -materialien, die alle Anforderungen des Merkmals „Brandverhalten“ erfüllen, ohne dass eine weitere Prüfung erforderlich ist, sind im Anhang aufgeführt.\\nArtikel 2\\nDie spezifischen Klassen, die im Rahmen der in der Entscheidung 2000/147/EG festgelegten Klassifizierung des Brandverhaltens für unterschiedliche Bauprodukte und/oder -materialien gelten, sind im Anhang aufgeführt.\\nArtikel 3\\nDie Produkte werden - sofern relevant - in Bezug auf ihre Endanwendung betrachtet.\\nArtikel 4\\nDiese Entscheidung ist an die Mitgliedstaaten gerichtet.\\nBrüssel, den 6. März 2006', 'labels': [1, 20, 7, 3, 0]}\n"
     ]
    }
   ],
   "source": [
    "# low_resource=['cs','sk']\n",
    "train_examples=[r for r in data_train]\n",
    "test_examples=[r for r in data_test]\n",
    "#print(type(train_examples))\n",
    "#print(len(train_examples))\n",
    "#print(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "701a6c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, TensorDataset\n",
    "class MetaTask(Dataset):\n",
    "    \n",
    "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
    "        \"\"\"\n",
    "        :param samples: list of samples\n",
    "        :param num_task: number of training tasks.\n",
    "        :param k_support: number of support sample per task\n",
    "        :param k_query: number of query sample per task\n",
    "        \"\"\"\n",
    "        self.examples = examples\n",
    "        random.shuffle(self.examples)\n",
    "        \n",
    "        self.num_task = num_task\n",
    "        self.k_support = k_support\n",
    "        self.k_query = k_query\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = 200       #\n",
    "        self.create_batch(self.num_task)\n",
    "    \n",
    "    def create_batch(self, num_task):\n",
    "        self.supports = []  # support set\n",
    "        self.queries = []  # query set\n",
    "        \n",
    "        for b in range(num_task):  # for each task\n",
    "            # 1.select domain randomly\n",
    "            #domain = random.choice(self.examples)['domain']   #domain corresponds to low resource language\n",
    "            #domainExamples = [e for e in self.examples if e['domain'] == domain] \n",
    "            #domainExamples=random.choice(self.examples)\n",
    "            domainExamples=random.sample(self.examples, 200)\n",
    "            # 1.select k_support + k_query examples from domain randomly\n",
    "            selected_examples = random.sample(list(domainExamples),self.k_support + self.k_query)\n",
    "            random.shuffle(selected_examples)\n",
    "            exam_train = selected_examples[:self.k_support]\n",
    "            exam_test  = selected_examples[self.k_support:]\n",
    "            \n",
    "            self.supports.append(exam_train)\n",
    "            self.queries.append(exam_test)\n",
    "\n",
    "    def create_feature_set(self,examples):\n",
    "        all_input_ids      = torch.empty((len(examples), self.max_seq_length), dtype = torch.long)\n",
    "        all_attention_mask = torch.empty((len(examples), self.max_seq_length), dtype = torch.long)\n",
    "        all_segment_ids    = torch.empty((len(examples), self.max_seq_length), dtype = torch.long)\n",
    "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
    "\n",
    "        for id_,example in enumerate(examples):\n",
    "            input_ids = tokenizer.encode(example['text'][:self.max_seq_length])\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            segment_ids    = [0] * len(input_ids)\n",
    "\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "            #while len(input_ids) < len(example['text']):\n",
    "                input_ids.append(0)\n",
    "                attention_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            #label_id = LABEL_MAP[example['label']].  ##check labels\n",
    "            label_id = LABEL_MAP[example['labels'][0]]\n",
    "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
    "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
    "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
    "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
    "\n",
    "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
    "        return tensor_set\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        support_set = self.create_feature_set(self.supports[index])\n",
    "        query_set   = self.create_feature_set(self.queries[index])\n",
    "        return support_set, query_set\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.num_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "745fe7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.TensorDataset at 0x7fc26a811c10>,\n",
       " <torch.utils.data.dataset.TensorDataset at 0x7fc272363590>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', do_lower_case = True)\n",
    "train = MetaTask(train_examples, num_task = 100, k_support=100, k_query=30, tokenizer = tokenizer)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d7bc15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing BertForSequenceClassification: ['roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'lm_head.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.embeddings.word_embeddings.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.pooler.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'classifier.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'classifier.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Task 0 ----\n",
      "Inner Loss:  4.237973724092756\n",
      "Inner Loss:  2.3726201908929005\n",
      "Inner Loss:  2.3076487268720354\n",
      "----Task 1 ----\n",
      "Inner Loss:  3.7493675776890347\n",
      "Inner Loss:  2.0911532981055125\n",
      "Inner Loss:  2.053628887448992\n",
      "Step: 0 \ttraining Acc: 0.45\n",
      "\n",
      "-----------------Testing Mode-----------------\n",
      "\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.3860280173165456\n",
      "Inner Loss:  2.349792957305908\n",
      "Inner Loss:  2.2937654427119663\n",
      "Inner Loss:  2.3204820496695384\n",
      "Inner Loss:  2.3693802867616927\n",
      "Inner Loss:  2.3483078479766846\n",
      "Inner Loss:  2.2813731942858015\n",
      "Inner Loss:  2.3149803706577847\n",
      "Inner Loss:  2.337707689830235\n",
      "Inner Loss:  2.260998708861215\n",
      "----Task 0 ----\n",
      "Inner Loss:  3.204136848449707\n",
      "Inner Loss:  2.0967569521495273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3861/3678161053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtest_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdb_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                     \u001b[0macc_all_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3861/1635567036.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch_tasks, training)\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0minner_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0minner_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner = Learner(args)\n",
    "    \n",
    "test = MetaTask(test_examples, num_task = args.num_task_test, k_support=args.k_spt,k_query=args.k_qry, tokenizer = tokenizer)\n",
    "\n",
    "global_step = 0\n",
    "#for epoch in range(args.epoch):\n",
    "for epoch in range(args.meta_epoch):\n",
    "        train = MetaTask(train_examples, num_task = args.num_task_train, k_support=args.k_spt, \n",
    "                         k_query=args.k_qry, tokenizer = tokenizer)\n",
    "\n",
    "        db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
    "\n",
    "        for step, task_batch in enumerate(db):\n",
    "\n",
    "            acc = learner(task_batch)\n",
    "\n",
    "            print('Step:', step, '\\ttraining Acc:', acc)\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                random_seed(123)\n",
    "                print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
    "                db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
    "                acc_all_test = []\n",
    "\n",
    "                for test_batch in db_test:\n",
    "                    acc = learner(test_batch, training = False)\n",
    "                    acc_all_test.append(acc)\n",
    "\n",
    "                print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
    "\n",
    "                random_seed(int(time.time() % 10))\n",
    "\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649e33c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
